{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/soft/anaconda/4.1.1/lib/python2.7/site-packages/IPython/kernel/__init__.py:13: ShimWarning: The `IPython.kernel` package has been deprecated. You should import from ipykernel or jupyter_client instead.\n",
      "  \"You should import from ipykernel or jupyter_client instead.\", ShimWarning)\n"
     ]
    }
   ],
   "source": [
    "import athg4_parser as g4\n",
    "import numpy as np\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import astropy.io.fits as pyfits\n",
    "import pandas as pd\n",
    "import random as rm\n",
    "from sklearn.cluster import DBSCAN\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g4dir = 'gcr_protons/gcr'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_summed_image_cutout_cent(deps):\n",
    "    x =[]\n",
    "    y = []\n",
    "    for part in deps:\n",
    "        for dep in part:\n",
    "            x.append(dep[0])\n",
    "            y.append(dep[1])\n",
    "    xmin, xmax = np.min(x), np.max(x)\n",
    "    ymin, ymax = np.min(y), np.max(y)\n",
    "    xcent, ycent = np.mean([xmin, xmax]), np.mean([ymin, ymax])\n",
    "    sz_x, sz_y = 50, 50\n",
    "    im = np.zeros((int(sz_x), int(sz_y)))\n",
    "    for part in deps:\n",
    "        for dep in part:\n",
    "            ix = int(dep[0]) - int(xcent) + int(sz_x/2)\n",
    "            iy = int(dep[1]) - int(ycent) + int(sz_y/2)\n",
    "            if ix >=0 and ix < sz_x and iy >= 0 and iy < sz_y:\n",
    "                im[ix, iy] += dep[2]*100  # offset pixels\n",
    "    return im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcr_protons/gcr/100_detector0\n",
      "gcr_protons/gcr/100_detector1\n",
      "gcr_protons/gcr/100_detector2\n",
      "gcr_protons/gcr/100_detector3\n",
      "gcr_protons/gcr/10_detector0\n",
      "gcr_protons/gcr/10_detector1\n",
      "gcr_protons/gcr/10_detector2\n",
      "gcr_protons/gcr/10_detector3\n",
      "gcr_protons/gcr/11_detector0\n",
      "gcr_protons/gcr/11_detector1\n"
     ]
    }
   ],
   "source": [
    "events = []\n",
    "num = 0\n",
    "for fn in sorted(glob.glob(g4dir + '/*')):\n",
    "    num += 1\n",
    "    if(num > 10):\n",
    "        break\n",
    "    print(fn)\n",
    "    try:\n",
    "        events += g4.parse(fn)\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "# put the events into a pandas dataframe\n",
    "gcr_df = g4.to_dataframe(events)\n",
    "gcr_dfg = gcr_df.groupby('eid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num = 0\n",
    "gcr_im = []\n",
    "gcr_labels = []\n",
    "for eid in set(gcr_df.eid):\n",
    "    im = get_summed_image_cutout_cent(gcr_dfg.get_group(eid).deps)\n",
    "        \n",
    "    num += 1\n",
    "        \n",
    "    gcr_im.append(im)\n",
    "    \n",
    "        \n",
    "gcr_im = np.array(gcr_im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "evlfile = 'test.raw'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = pyfits.open(evlfile)\n",
    "df = pd.DataFrame({'FRAME' : f['EVENTS'].data['FRAME'].byteswap().newbyteorder('<'),\n",
    "                   'PHA' : f['EVENTS'].data['PHA'].byteswap().newbyteorder('<'),\n",
    "                   'RAWX' : f['EVENTS'].data['RAWX'].byteswap().newbyteorder('<'),\n",
    "                   'RAWY' : f['EVENTS'].data['RAWY'].byteswap().newbyteorder('<'),\n",
    "                   })\n",
    "f.close()\n",
    "df_byframe = df.groupby('FRAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_frame_image(fr):\n",
    "    x = []\n",
    "    y = []\n",
    "    for i,r in df_byframe.get_group(fr).iterrows():\n",
    "        x.append(r.RAWX)\n",
    "        y.append(r.RAWY)\n",
    "    xmin, xmax = np.min(x), np.max(x)\n",
    "    ymin, ymax = np.min(y), np.max(y)\n",
    "    xcent, ycent = np.mean([xmin, xmax]), np.mean([ymin, ymax])\n",
    "    sz_x, sz_y = 50, 50\n",
    "    im = np.zeros((int(sz_x), int(sz_y)))\n",
    "    for i,r in df_byframe.get_group(fr).iterrows():\n",
    "        ix = r.RAWX - int(xcent) + int(sz_x/2)\n",
    "        iy = r.RAWY - int(ycent) + int(sz_y/2)\n",
    "        if ix >=0 and ix < sz_x and iy >= 0 and iy < sz_y:\n",
    "            im[ix, iy] += r.PHA  # offset pixels\n",
    "    return im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num = 0\n",
    "xr_im = []\n",
    "xr_labels = []\n",
    "lim = 20000\n",
    "for fr in set(df.FRAME):\n",
    "    num += 1\n",
    "    if num > lim:\n",
    "        break\n",
    "    im = get_frame_image(fr)\n",
    "        \n",
    "    xr_im.append(im)\n",
    "xr_im=np.array(xr_im)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I generate a train set randomly. Each figure contains 0-1 photon and 0-2 particles, which is recognized as a single cluster by DBSCAN. The label for a figure containing a photon is 1, otherwise the label is 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "count=0\n",
    "photon_index=0\n",
    "particle_index=0\n",
    "no_conn_num=0\n",
    "train_im=[]\n",
    "train_label=[]\n",
    "while True:\n",
    "    photon_num=rm.randint(0,1)\n",
    "    particle_num=rm.randint(0,2)\n",
    "    while photon_num==0 and particle_num==0:\n",
    "        photon_num=rm.randint(0,1)\n",
    "        particle_num=rm.randint(0,2)\n",
    "    if photon_num==1:\n",
    "        while np.nonzero(xr_im[photon_index])[0].shape[0]==0:\n",
    "            photon_index+=1\n",
    "        train_im.append(xr_im[photon_index].copy())\n",
    "        photon_index+=1\n",
    "        train_label.append(1)\n",
    "    else:\n",
    "        train_im.append(np.zeros((50,50)))\n",
    "        train_label.append(0)\n",
    "    if particle_num>=1:\n",
    "        while np.nonzero(gcr_im[particle_index])[0].shape[0]==0:\n",
    "            particle_index+=1\n",
    "        dx=rm.randint(-3,3)\n",
    "        dy=rm.randint(-3,3)\n",
    "        im_shift=np.roll(gcr_im[particle_index],dx,axis=1)\n",
    "        im_shift=np.roll(im_shift,dy,axis=0)\n",
    "        train_im[count]+=im_shift\n",
    "        particle_index+=1\n",
    "    if particle_num==2:\n",
    "        while np.nonzero(gcr_im[particle_index])[0].shape[0]==0:\n",
    "            particle_index+=1\n",
    "        dx=rm.randint(-3,3)\n",
    "        dy=rm.randint(-3,3)\n",
    "        im_shift=np.roll(gcr_im[particle_index],dx,axis=1)\n",
    "        im_shift=np.roll(im_shift,dy,axis=0)\n",
    "        train_im[count]+=im_shift\n",
    "        particle_index+=1\n",
    "    y_max=np.nonzero(train_im[count])[0].max()\n",
    "    y_min=np.nonzero(train_im[count])[0].min()\n",
    "    x_max=np.nonzero(train_im[count])[1].max()\n",
    "    x_min=np.nonzero(train_im[count])[1].min()\n",
    "    dx=-int((x_max+x_min)/2)+25\n",
    "    dy=-int((y_max+y_min)/2)+25\n",
    "    train_im[count]=np.roll(train_im[count],dx,axis=1)\n",
    "    train_im[count]=np.roll(train_im[count],dy,axis=0)\n",
    "    count+=1\n",
    "    \n",
    "    X=np.transpose(np.nonzero(train_im[count-1]))\n",
    "    clustering = DBSCAN(eps=3, min_samples=3).fit(X)\n",
    "    \n",
    "    \n",
    "    if np.unique(clustering.labels_).shape[0]>1:\n",
    "        no_conn_num+=1\n",
    "        count-=1\n",
    "        if no_conn_num<8:\n",
    "            photon_index-=photon_num\n",
    "            particle_index-=particle_num\n",
    "        del train_im[count]\n",
    "        del train_label[count]\n",
    "    else:\n",
    "        no_conn_num=0\n",
    "    \n",
    "    if particle_index>=10000:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_im=np.array(train_im)\n",
    "train_label=np.array(train_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The X-ray signal is much weaker than the particle signal. Therefore, in order to increase the prediction accuracy in later classification by deep learning, I set all non-zero elements of the train set to be a constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_im_all1=np.copy(train_im)\n",
    "train_im_all1[np.nonzero(train_im_all1)]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 2500)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               640256    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 640,513\n",
      "Trainable params: 640,513\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "8083/8083 [==============================] - 2s 193us/step - loss: 0.1991 - acc: 0.9065\n",
      "Epoch 2/5\n",
      "8083/8083 [==============================] - 1s 123us/step - loss: 0.0458 - acc: 0.9875\n",
      "Epoch 3/5\n",
      "8083/8083 [==============================] - 1s 123us/step - loss: 0.0202 - acc: 0.9968\n",
      "Epoch 4/5\n",
      "8083/8083 [==============================] - 1s 122us/step - loss: 0.0120 - acc: 0.9980\n",
      "Epoch 5/5\n",
      "8083/8083 [==============================] - 1s 121us/step - loss: 0.0080 - acc: 0.9991\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f5a4e012cd0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(50, 50)),\n",
    "    keras.layers.Dense(256, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer=tf.train.AdamOptimizer(), \n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#train_im = train_im.reshape(7500,50,50,1)\n",
    "#test_im = test_im.reshape(4500,50,50,1)\n",
    "model.fit(train_im_all1, train_label, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I generate randomly a 500*500 screen of the detector before eliminating noises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I use DBSCAN algorithm to cluster the screen and use the deep learning model I previously trained to predict whether each cluser contains a photon or not. If yes, the cluster will be kept, otherwise the cluster will be deleted from the screen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def denoise(screen):\n",
    "    X=np.transpose(np.nonzero(screen))\n",
    "    clustering = DBSCAN(eps=3, min_samples=3).fit(X)\n",
    "    test_im=[]\n",
    "    for value in np.unique(clustering.labels_):\n",
    "        if value==-1:\n",
    "            continue\n",
    "        y_max=np.nonzero(screen)[0][clustering.labels_==value].max()\n",
    "        y_min=np.nonzero(screen)[0][clustering.labels_==value].min()\n",
    "        x_max=np.nonzero(screen)[1][clustering.labels_==value].max()\n",
    "        x_min=np.nonzero(screen)[1][clustering.labels_==value].min()\n",
    "        dx=25-int((x_max+x_min)/2)\n",
    "        dy=25-int((y_max+y_min)/2)\n",
    "        im=np.zeros((50,50))  \n",
    "        if (np.nonzero(screen)[0][clustering.labels_==value]+dy>=50).sum()+(np.nonzero(screen)[1][clustering.labels_==value]+dx>=50).sum()==0:\n",
    "            im[np.nonzero(screen)[0][clustering.labels_==value]+dy,np.nonzero(screen)[1][clustering.labels_==value]+dx]=screen[np.nonzero(screen)[0][clustering.labels_==value],np.nonzero(screen)[1][clustering.labels_==value]]\n",
    "        test_im.append(im)\n",
    "    test_im=np.array(test_im)\n",
    "    test_im_all1=np.copy(test_im)\n",
    "    test_im_all1[np.nonzero(test_im_all1)]=1\n",
    "\n",
    "    predict=np.transpose(model.predict(test_im_all1,verbose=0))[0]\n",
    "    screen1=np.copy(screen)\n",
    "    for value in np.unique(clustering.labels_)[1:][predict<=0.5]:\n",
    "        screen1[np.nonzero(screen)[0][clustering.labels_==value],np.nonzero(screen)[1][clustering.labels_==value]]=0\n",
    "    return screen1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_screen=[]\n",
    "test_screen_no_noise=[]\n",
    "for i in range(1000):\n",
    "    screen=np.zeros((500,500))\n",
    "    for im_x_num in range(50):\n",
    "        im=xr_im[rm.randint(0,15000)]\n",
    "        x_x=rm.randint(0,450)\n",
    "        y_x=rm.randint(0,450)\n",
    "        screen[x_x:x_x+50,y_x:y_x+50]+=im\n",
    "    test_screen_no_noise.append(screen.copy())\n",
    "    for im_g_num in range(50):\n",
    "        im=gcr_im[rm.randint(0,15000)]\n",
    "        if len(im.nonzero()[0])==0 or im.nonzero()[0].max()-im.nonzero()[0].min()>40 or im.nonzero()[1].max()-im.nonzero()[1].min()>40:\n",
    "            continue\n",
    "        x_g=rm.randint(0,450)\n",
    "        y_g=rm.randint(0,450)\n",
    "        screen[x_g:x_g+50,y_g:y_g+50]+=im\n",
    "    test_screen.append(screen.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0289773683425\n"
     ]
    }
   ],
   "source": [
    "errors=0\n",
    "for i in range(1000):\n",
    "    screen=test_screen[i]\n",
    "    screen_perfect=test_screen_no_noise[i]\n",
    "    #print(i)\n",
    "    screen1=denoise(screen)\n",
    "    screen1_co=screen1.nonzero()[0]*500+screen1.nonzero()[1]\n",
    "    screen_per_co=screen_perfect.nonzero()[0]*500+screen_perfect.nonzero()[1]\n",
    "    error=float(len(np.setdiff1d(screen1_co,screen_per_co))+len(np.setdiff1d(screen_per_co, screen1_co)))/len(screen_perfect.nonzero()[0])\n",
    "    errors+=error/1000\n",
    "print(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
